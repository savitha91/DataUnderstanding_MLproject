{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset : IOT Sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim of the project\n",
    "We are going to apply stats to get list of input variables that are independent of each other and dependent on the target variable",
    "1. Dependent variable - Target variable : Vehicle\n",
    "2. Independent variables - Input variables : DateTime, Junction, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Junction</th>\n",
       "      <th>Vehicles</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>20151101001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>20151101011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>20151101021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DateTime  Junction  Vehicles           ID\n",
       "0  2015-11-01 00:00:00         1        15  20151101001\n",
       "1  2015-11-01 01:00:00         1        13  20151101011\n",
       "2  2015-11-01 02:00:00         1        10  20151101021"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"IOT_train.csv\")\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48120 entries, 0 to 48119\n",
      "Data columns (total 4 columns):\n",
      "DateTime    48120 non-null object\n",
      "Junction    48120 non-null int64\n",
      "Vehicles    48120 non-null int64\n",
      "ID          48120 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Convert DateTime column of type object  to  type Date time , to get more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['DateTime'] = pd.to_datetime(df_train['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Month'] = df_train.DateTime.dt.month\n",
    "df_train['Day']= df_train.DateTime.dt.day #Day of month\n",
    "df_train['Time']= (((df_train.DateTime.dt.hour * 60 )+ df_train.DateTime.dt.minute) * 60 ) + df_train.DateTime.dt.second\n",
    "df_train['Weekday']= df_train.DateTime.dt.weekday\n",
    "df_train['Week']= df_train.DateTime.dt.week  #Week of year\n",
    "df_train['Quarter']= df_train.DateTime.dt.quarter #Date belongs to which quarter of the year. Eg: Oct-Nov-Dec belong to quarter4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Date with unixtime list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetounix(df):\n",
    "    # Initialising unixtime list\n",
    "    unixtime = []\n",
    "    \n",
    "    # Running a loop for converting Date to seconds\n",
    "    for date in df['DateTime']:\n",
    "        unixtime.append(time.mktime(date.timetuple()))\n",
    "    \n",
    "    # Replacing Date with unixtime list\n",
    "    df['DateTime'] = unixtime\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = datetounix(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Junction</th>\n",
       "      <th>Vehicles</th>\n",
       "      <th>ID</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Week</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.446316e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>20151101001</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.446320e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>20151101011</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DateTime  Junction  Vehicles           ID  Month  Day  Time  Weekday  \\\n",
       "0  1.446316e+09         1        15  20151101001     11    1     0        6   \n",
       "1  1.446320e+09         1        13  20151101011     11    1  3600        6   \n",
       "\n",
       "   Week  Quarter  \n",
       "0    44        4  \n",
       "1    44        4  "
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48120 entries, 0 to 48119\n",
      "Data columns (total 10 columns):\n",
      "DateTime    48120 non-null float64\n",
      "Junction    48120 non-null int64\n",
      "Vehicles    48120 non-null int64\n",
      "ID          48120 non-null int64\n",
      "Month       48120 non-null int64\n",
      "Day         48120 non-null int64\n",
      "Time        48120 non-null int64\n",
      "Weekday     48120 non-null int64\n",
      "Week        48120 non-null int64\n",
      "Quarter     48120 non-null int64\n",
      "dtypes: float64(1), int64(9)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert those features with finite number of possible values to Categorical data.\n",
    "#### Advantage : Gives more info, uses less memory which leads to performance improvement\n",
    "\n",
    "Ref: https://pbpython.com/pandas_dtypes_cat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48120 entries, 0 to 48119\n",
      "Data columns (total 10 columns):\n",
      "DateTime    48120 non-null float64\n",
      "Junction    48120 non-null category\n",
      "Vehicles    48120 non-null int64\n",
      "ID          48120 non-null int64\n",
      "Month       48120 non-null int64\n",
      "Day         48120 non-null int64\n",
      "Time        48120 non-null int64\n",
      "Weekday     48120 non-null category\n",
      "Week        48120 non-null int64\n",
      "Quarter     48120 non-null category\n",
      "dtypes: category(3), float64(1), int64(6)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Make Junction, Weekday, Quarter columns as categorical\n",
    "df_train['Junction'] = df_train['Junction'].astype('category')\n",
    "df_train['Weekday'] = df_train['Weekday'].astype('category')\n",
    "df_train['Quarter'] = df_train['Quarter'].astype('category')\n",
    "\n",
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of continuous and categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_Vars = df_train.select_dtypes(exclude=['object','category']).columns\n",
    "categorical_Vars = df_train.select_dtypes(include=['object','category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we remove ID variable from continuousVar list\n",
    "\"\"\"\n",
    "varList =[]\n",
    "def dropVar(var,continuous_Vars):\n",
    "    for i in continuous_Vars:\n",
    "        if var in i:\n",
    "            continuous_Vars = continuous_Vars.drop(i)\n",
    "    return continuous_Vars\n",
    "\n",
    "\n",
    "continuous_Vars = dropVar(\"ID\", continuous_Vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DateTime', 'Vehicles', 'Month', 'Day', 'Time', 'Week'], dtype='object')"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "continuous_Vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis  :: Continuous variables \n",
    "\n",
    "### Aim : To identify any outliers / skewness\n",
    "##### Techniques that can be used :\n",
    "1. central tendency measure (or)\n",
    "2. box plot (or)\n",
    "3. distibution/hist plot (or)\n",
    "\n",
    "#### Inference: We can get an idea of outliers i.e whether data is skewed to left/right\n",
    "1. Data skewed to left : More outliers at lower end of data :: Mean < Median/Mode\n",
    "2. Data skewed to right : More outliers at higher end of data :: Mean > Median/Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime  column is skewed to left i.e. More outliers at lower values\n",
      "Vehicles  is skewed to right i.e. More outliers at higher values\n",
      "Month  is skewed to right i.e. More outliers at higher values\n",
      "Day  column is skewed to left i.e. More outliers at lower values\n",
      "Week  is skewed to right i.e. More outliers at higher values\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note:  Here we are not considering Continuous variable\n",
    "\"\"\"\n",
    "def univar_analysis_continuous(continuous_Vars):\n",
    "    for col in continuous_Vars:\n",
    "        if(df_train[col].mean() > df_train[col].median()):\n",
    "            print(col , \" is skewed to right i.e. More outliers at higher values\")\n",
    "        elif (df_train[col].mean() < df_train[col].median()):\n",
    "            print(col, \" column is skewed to left i.e. More outliers at lower values\")\n",
    "        #plt.boxplot(df1_train['col'])\n",
    "        #plt.hist(df_train[col])\n",
    "            \n",
    "univar_analysis_continuous(continuous_Vars[:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : These are population data. We need to take samples( of sample size > 30) and check if the sample distribution is normal.  Hypothesis Test can be performed on these samples using z-test or t-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis  :: Categorical variables \n",
    "\n",
    "### Aim: To identify count /  freq distribution\n",
    "#### Techniques that can be used :\n",
    "1. Frequence table - Crosstab : can be used for continuous and categorical data types\n",
    "2. Bar plot\n",
    "3. Count plot - seaborn\n",
    "4. value_counts()\n",
    "\n",
    "Ref : https://www.kaggle.com/hamelg/python-for-data-19-frequency-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around  30.0 % data in  JUNCTION  column belongs to the category :  1\n",
      "Around  14.0 % data in  WEEKDAY  column belongs to the category :  0\n",
      "Around  31.0 % data in  QUARTER  column belongs to the category :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarames/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:56: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def univar_analysis_categorical(obj_col_list):\n",
    "    for i in obj_col_list:\n",
    "        freq = pd.crosstab(index=df_train[i], columns='count')\n",
    "        percent_val= (freq/np.sum(freq))*100\n",
    "        print(\"Around \", np.floor(np.max(percent_val['count'])) ,\"% data in \",i.upper(),\" column belongs to the category : \",np.argmax(percent_val['count']))\n",
    "        # plt.bar(freq.index,freq['count'])\n",
    "        # sns.countplot(df1_train[i])\n",
    "    \n",
    "univar_analysis_categorical(categorical_Vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-variate analysis : Test for dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get variable pairs \n",
    "1. Continuous-Continuous Variable pairs\n",
    "2. Continuous-Categorical Variable pairs\n",
    "3. Categorical-Categorical Variable pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def get_ContinuousVar_Pairs(continuous_Var):\n",
    "    continuousVar_pairs  = []\n",
    "    for i in range(len(continuous_Var)):\n",
    "        for j  in range(i+1,len(continuous_Var)):\n",
    "            continuousVar_pairs.append((continuous_Var[i], continuous_Var[j]))\n",
    "    return continuousVar_pairs\n",
    "            \n",
    "def get_CategoricalVar_Pairs(categorical_Var):\n",
    "    categoricalVar_pairs  = []\n",
    "    for i in range(len(categorical_Var)):\n",
    "        for j  in range(i+1,len(categorical_Var)):\n",
    "            categoricalVar_pairs.append((categorical_Var[i], categorical_Var[j]))\n",
    "    return categoricalVar_pairs\n",
    "\n",
    "            \n",
    "def get_Categ_Conti_Pairs(categorical_Var,continuous_Var):\n",
    "    categ_ContiVar_pairs  = []\n",
    "    for i in range(len(continuous_Var)):\n",
    "        for j  in range(len(categorical_Var)):\n",
    "            categ_ContiVar_pairs.append((continuous_Var[i], categorical_Var[j]))\n",
    "    return categ_ContiVar_pairs\n",
    "\n",
    "continuousVar_pairs = get_ContinuousVar_Pairs(continuous_Vars)\n",
    "categoricalVar_pairs = get_CategoricalVar_Pairs(categorical_Vars)\n",
    "categ_ContVar_pairs = get_Categ_Conti_Pairs(categorical_Vars,continuous_Vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list to store dependent and independent variables\n",
    "dependent_var = []\n",
    "independent_var = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency test expected results\n",
    "\n",
    "1. Input variables should be independent of each other\n",
    "    > If input variables are dependent, we need to consider only one of the two variables\n",
    "2. Input variables should be dependent on output variable.\n",
    "    > If input variables is independent on output variable, we should ignore that independent variable while modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency test on Continuous-Categorical Pair\n",
    "\n",
    "1. Dependent variables : Continuous\n",
    "2. Independent variables : categorical\n",
    "\n",
    "\n",
    "#### Eg: Say we  need to find if there is any difference in training hour(Dependent var) of Male and Female , who have taken the same online course. This tells us if there is a dependency between Gender and TrainingHour. Null Hypothesis assumes no dependency  between variables (i.e., No difference in training hours between Male and Female). Alternate Hyp assumes there is dependency\n",
    "\n",
    "1. H0 : Assume no difference in the training hours between Male(M) and Female(F) , Mean_training_F = Mean_training_M\n",
    "2. H1 : There is difference. Mean_training_F ≠ Mean_training_M\n",
    "    \n",
    "## Anova : Analysis of Variance test\n",
    " \n",
    "#### Steps done internally by stats.f_oneway\n",
    "\n",
    "1. Calculate within group and between groups variability\n",
    "2. Calculate F-Ratio\n",
    "3. Calculate probability using F-table\n",
    "\n",
    "### What do we infer ?\n",
    "If the mean values between the categories are very far and if mean value of one category falls within the rejection area(given by critical_p_value = alpha) ,we reject null hypothesis(i.e favour alternate hyp) and say that the mean difference between the categories are significant. i.e. there is significant difference between the categories. With this we conclude that Varaiables are dependent on each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "### H0: Variables are independent of each other. \n",
    "### H1: Variables are dependent on each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We apply one-way ANOVA test(stats.f_oneway) on the grouped categories. This results in tuple (f-ratio and p_value)\n",
    "We set critical_p_value(alpha) as 0.05 - One tailed test\n",
    "\n",
    "We need to identify 2 Degrees of freedom(DF)::\n",
    "> Numerator DF(m) = Number of categories -1\n",
    "> Denominator DF = m * [size of data -1](Each category will have different size. Here we consider max datasize)\n",
    "We calculate corresponding critical_f_ratio using stats.f.ppf, passing alpha and DF\n",
    "\n",
    "Here we test the hypothesis by 2 methods- (You can consider one of the techniques)\n",
    "1. critical_p_value(alpha) vs p_value : If p_value is less than alpha, i.e. one of the categories mean falls within rejection,\n",
    "area , we reject null hypothesis\n",
    "2. critical_f_ratio vs f_ratio : If f_ratio is greater than critical_f_ratio, i.e. one of the categories mean falls within rejection,\n",
    "area , we reject null hypothesis\n",
    "\n",
    "When we reject null hypothesis => we accept alternate hypothesis and conclude that the variables are dependent and \n",
    "add input variables to dependent_var list\n",
    "Similarly we add independent variables to independent_var list\n",
    "\"\"\"\n",
    "def anovaTest(categories,contCateg,categColumnsDf):\n",
    "    \n",
    "    result = stats.f_oneway(*categories)\n",
    "    m = len(categColumnsDf.columns)-1\n",
    "    n = categColumnsDf.shape[0] - 1\n",
    "    f_ratio = result[0]\n",
    "    #Calculate p_value from f_ratio : https://www.socscistatistics.com/pvalues/fdistribution.aspx\n",
    "    p_value = result[1]\n",
    "    # Calculate f-ratio from p-value(alpha) : http://www.biokin.com/tools/f-critical.html\n",
    "    critical_p_value = alpha = 0.05 #Rejection area for one-tailed(right) test\n",
    "    critical_f_ratio = stats.f.ppf(q=1-alpha, dfn=m, dfd= (m*n))\n",
    "   \n",
    "    if(f_ratio > critical_f_ratio):\n",
    "        z_result = \"Reject null Hyp\"\n",
    "    else:\n",
    "        z_result = \"Fail to reject null Hyp\"\n",
    "\n",
    "\n",
    "    if(p_value <= critical_p_value):\n",
    "        p_val_result = \"Reject null Hyp\"\n",
    "    else:\n",
    "        p_val_result = \"Fail to reject null Hyp\"\n",
    "        \n",
    "\n",
    "    if(z_result ==  p_val_result ==  \"Fail to reject null Hyp\"):\n",
    "        print(contCateg[0], \"and\", contCateg[1], \":: INDEPENDENT\")\n",
    "        independent_var.append((contCateg[0],contCateg[1]))\n",
    "    elif(z_result ==  p_val_result ==  \"Reject null Hyp\"):\n",
    "        print(\"****\",contCateg[0], \"and\", contCateg[1], \":: DEPENDENT *****\")\n",
    "        dependent_var.append((contCateg[0],contCateg[1]))\n",
    "        #print(\"Average training hours of male and female are statistically different. Hence, training hour depends on gender\")\n",
    "    else:\n",
    "        print(\"Error :: Please check the results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** DateTime and Junction :: DEPENDENT *****\n",
      "DateTime and Weekday :: INDEPENDENT\n",
      "**** DateTime and Quarter :: DEPENDENT *****\n",
      "**** Vehicles and Junction :: DEPENDENT *****\n",
      "**** Vehicles and Weekday :: DEPENDENT *****\n",
      "**** Vehicles and Quarter :: DEPENDENT *****\n",
      "**** Month and Junction :: DEPENDENT *****\n",
      "Month and Weekday :: INDEPENDENT\n",
      "**** Month and Quarter :: DEPENDENT *****\n",
      "Day and Junction :: INDEPENDENT\n",
      "Day and Weekday :: INDEPENDENT\n",
      "Day and Quarter :: INDEPENDENT\n",
      "Time and Junction :: INDEPENDENT\n",
      "Time and Weekday :: INDEPENDENT\n",
      "Time and Quarter :: INDEPENDENT\n",
      "**** Week and Junction :: DEPENDENT *****\n",
      "Week and Weekday :: INDEPENDENT\n",
      "**** Week and Quarter :: DEPENDENT *****\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We take the continuous-categorical pair and group them based on categories.\n",
    "Apply ANOVA test on the grouped data\n",
    "\"\"\"\n",
    "def analyse_Conti_Categ(categ_ContVar_pairs):\n",
    "    for contCateg in categ_ContVar_pairs:\n",
    "        categVar = df_train[contCateg[1]]\n",
    "        contVar = df_train[contCateg[0]]\n",
    "        categ_cont_Df = pd.DataFrame({contCateg[1] : categVar, contCateg[0]: contVar })\n",
    "        grouped = categ_cont_Df.groupby(contCateg[1]).groups\n",
    "        newDf = pd.DataFrame.from_dict(grouped, orient='index')\n",
    "        categColumnsDf = newDf.transpose()\n",
    "        allCategoriesData =[]\n",
    "        for i in grouped:\n",
    "            eachCategoryData= contVar[grouped[i]]\n",
    "            allCategoriesData.append(eachCategoryData)\n",
    "        anovaTest(allCategoriesData,contCateg,categColumnsDf)\n",
    "       \n",
    "        \n",
    "analyse_Conti_Categ(categ_ContVar_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency test on Categorical-Categorical Pair\n",
    "\n",
    "Ref: https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/test-of-independence-2-of-3/\n",
    "\n",
    "We consider Frequency distribution of data between categorical variables using crosstab function\n",
    "The frequency of each category for one nominal variable is compared across the categories of the second nominal variable.\n",
    "\n",
    "If conditional percentages for one category differ from other, this says there is dependency. But do they differ enough to conclude strongly that variables are dependent? For this we go ahead with Hypothesis testing\n",
    "\n",
    "## Chi-Square Test\n",
    "Chi-Square is called a test of independence because “no relationship” means “independent.” If there is a relationship between the two variables in the population, then they are dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Hypothesis\n",
    "### H0: Variables are  independent of each other i.e no relationship between categories.\n",
    "### H1: Variables are dependent on each other i.e there exists relationship between categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we obtained observed count and expected count from the distribution table\n",
    "If the difference between observed count and expected count is large, we conclude that the variables are dependent on each other\n",
    "If observed count falls under rejection region, we reject null hypothesis and coclude that the variables are dependent\n",
    "\n",
    "Degrees Of freedom = (number of categories in categoricalVar1 – 1) × (number of response categories in categoricalVar2 – 1)\n",
    "\n",
    "We test the hypothesis by 2 methods- (You can consider one of the techniques)\n",
    "1. critical_p_value(alpha) vs p_value : If p_value is less than alpha, i.e. one of the categories mean falls within rejection,\n",
    "area , we reject null hypothesis\n",
    "2. critical_f_ratio vs f_ratio : If f_ratio is greater than critical_f_ratio, i.e. one of the categories mean falls within rejection,\n",
    "area , we reject null hypothesis\n",
    "\n",
    "When we reject null hypothesis => we accept alternate hypothesis and conclude that the variables are dependent and \n",
    "add input variables to dependent_var list\n",
    "Similarly we add independent variables to independent_var list\n",
    "\"\"\"\n",
    "from scipy import stats\n",
    "def chisq(pair,var0,var1):\n",
    "    var0_count = len(pair.index)-1\n",
    "    var1_count = len(pair.columns)-1\n",
    "    observedDf = pair.iloc[:var0_count,:var1_count]\n",
    "    expected = np.outer(pair['Total'][:var0_count], pair.loc['Total'][:var1_count])/pair.iloc[var0_count,var1_count]\n",
    "    expectedDf = pd.DataFrame(expected, index=observedDf.index, columns= observedDf.columns)\n",
    "    chisq_value = (np.square(observedDf - expectedDf)/expectedDf).sum().sum()\n",
    "    df = (len(df_train[var0].value_counts().index)-1) *(len(df_train[var1].value_counts().index)-1)\n",
    "    alpha = critical_p_value =  0.05\n",
    "    critical_Chisq_value = stats.chi2.ppf(q = 0.95,df = df) \n",
    "   \n",
    "    p_value = 1 - stats.chi2.cdf(x=chisq_value,  # Find the p-value\n",
    "                             df=df)\n",
    "    \n",
    "    if(chisq_value > critical_Chisq_value):\n",
    "        z_result = \"Reject null Hyp\"\n",
    "    else:\n",
    "        z_result = \"Fail to reject null Hyp\"\n",
    "   \n",
    "    if(p_value <= critical_p_value):\n",
    "        p_val_result = \"Reject null Hyp\"\n",
    "    else:\n",
    "        p_val_result = \"Fail to reject null Hyp\"\n",
    "    \n",
    "    if(z_result ==  p_val_result ==  \"Fail to reject null Hyp\"):\n",
    "        print(var0, \"and\", var1, \" :: INDEPENDENT\")\n",
    "        independent_var.append((var0,var1))\n",
    "    elif(z_result ==  p_val_result ==  \"Reject null Hyp\"):\n",
    "        print(\"****\",var0, \"and\", var1, \" :: DEPENDENT\")\n",
    "        dependent_var.append((var0,var1))\n",
    "    else:\n",
    "        print(\"Error :: Please check the results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junction and Weekday  :: INDEPENDENT\n",
      "**** Junction and Quarter  :: DEPENDENT\n",
      "Weekday and Quarter  :: INDEPENDENT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyseCateg(categoricalVar_pairs):\n",
    "    for categVar in categoricalVar_pairs:\n",
    "        pair = pd.crosstab(index=df_train[categVar[0]], columns=df_train[categVar[1]], margins=True,margins_name='Total')\n",
    "        chisq(pair,categVar[0],categVar[1])        \n",
    "        \n",
    "analyseCateg(categoricalVar_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency test on Continuous-Continuous Pair\n",
    "\n",
    "We calculate correlation between continuous variables\n",
    "\n",
    "## PearsonR \n",
    "Calculates Pearson correlation coefficient and the p-value for testing non-correlation\n",
    "\n",
    "Larger r-square, better the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "### H0: Variables are  independent of each other i.e Slope is zero.\n",
    "### H1: Variables are dependent on each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** DateTime  and  Vehicles :: DEPENDENT ****\n",
      "**** DateTime  and  Month :: DEPENDENT ****\n",
      "**** DateTime  and  Day :: DEPENDENT ****\n",
      "DateTime  and  Time :: INDEPENDENT\n",
      "**** DateTime  and  Week :: DEPENDENT ****\n",
      "**** Vehicles  and  Month :: DEPENDENT ****\n",
      "**** Vehicles  and  Day :: DEPENDENT ****\n",
      "**** Vehicles  and  Time :: DEPENDENT ****\n",
      "**** Vehicles  and  Week :: DEPENDENT ****\n",
      "**** Month  and  Day :: DEPENDENT ****\n",
      "Month  and  Time :: INDEPENDENT\n",
      "**** Month  and  Week :: DEPENDENT ****\n",
      "Day  and  Time :: INDEPENDENT\n",
      "**** Day  and  Week :: DEPENDENT ****\n",
      "Time  and  Week :: INDEPENDENT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we consider two continuous variables and find the correlation value and pval\n",
    "If pval < alpha, we reject null hypothesis and conclude the variables are dependent on each other\n",
    "\"\"\"\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "def test_biVariate_continuousVar_DependencyTest(continuousVar_pairs):\n",
    "    for contVar in continuousVar_pairs: \n",
    "        if(contVar[0]!='ID'):\n",
    "            rval , pval = pearsonr(df_train[contVar[0]],df_train[contVar[1]])\n",
    "            if(pval < 0.05):\n",
    "                dependent_var.append((contVar[0],contVar[1]))\n",
    "                print(\"****\",contVar[0],\" and \" , contVar[1], \":: DEPENDENT\",\"****\")\n",
    "            else:\n",
    "                independent_var.append((contVar[0],contVar[1]))\n",
    "                print(contVar[0],\" and \" , contVar[1], \":: INDEPENDENT\")\n",
    "            \n",
    "test_biVariate_continuousVar_DependencyTest(continuousVar_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all features from dataset. Remove ID column\n",
    "\"\"\"\n",
    "features = df_train.columns.values\n",
    "features = features[features != 'ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DateTime', 'Junction', 'Vehicles', 'Month', 'Day', 'Time',\n",
       "       'Weekday', 'Week', 'Quarter'], dtype=object)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aim is to remove input variables that are independent of target variable(Vehicle) from features list\n",
    "\"\"\"\n",
    "def removeFeatures_IndependentToTarget(target,remove_features,independent_var,features):\n",
    "    for var in independent_var:\n",
    "        if(target in var[0]):\n",
    "            remove_features.append(var[1])\n",
    "            #print(var[1],\" is independent of target. Hence, this var can be removed from features list\")\n",
    "        elif(target in var[1]):\n",
    "            remove_features.append(var[0])\n",
    "            #print(var[0],\" is independent of target. Hence, this var can be removed from features list\")\n",
    "\n",
    "    for i in remove_features:\n",
    "        features = np.delete(features, np.where(features == i))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Vehicles'\n",
    "remove_features = ['Vehicles']\n",
    "features = removeFeatures_IndependentToTarget(target,remove_features,independent_var,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original set of input features considered :  ['DateTime' 'Junction' 'Month' 'Day' 'Time' 'Weekday' 'Week' 'Quarter']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original set of input features considered : \",features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature list with combinations of independent input variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['DateTime', 'Weekday', 'Time'],\n",
       " ['Junction', 'Day', 'Time', 'Weekday'],\n",
       " ['Month', 'Weekday', 'Time'],\n",
       " ['Day', 'Junction', 'Weekday', 'Quarter', 'Time'],\n",
       " ['Time',\n",
       "  'Junction',\n",
       "  'Weekday',\n",
       "  'Quarter',\n",
       "  'DateTime',\n",
       "  'Month',\n",
       "  'Day',\n",
       "  'Week'],\n",
       " ['Weekday',\n",
       "  'DateTime',\n",
       "  'Month',\n",
       "  'Day',\n",
       "  'Time',\n",
       "  'Week',\n",
       "  'Junction',\n",
       "  'Quarter'],\n",
       " ['Week', 'Weekday', 'Time'],\n",
       " ['Quarter', 'Day', 'Time', 'Weekday']]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider each item from  feature list, add all features that are independent to this item\n",
    "independentFeatures= []\n",
    "for f in features:\n",
    "    tempFeatures = []\n",
    "    tempFeatures.append(f)\n",
    "    for d in independent_var:\n",
    "        if(f == d[0]): \n",
    "            tempFeatures.append(d[1])\n",
    "        elif(f == d[1]):\n",
    "            tempFeatures.append(d[0])\n",
    "    independentFeatures.append(tempFeatures)       \n",
    "            \n",
    "print(\"Feature list with combinations of independent input variables\")\n",
    "independentFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing those features that are same as originalset of input features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarames/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \"\"\"\n",
      "/Users/sarames/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['DateTime', 'Weekday', 'Time'],\n",
       " ['Junction', 'Day', 'Time', 'Weekday'],\n",
       " ['Month', 'Weekday', 'Time'],\n",
       " ['Day', 'Junction', 'Weekday', 'Quarter', 'Time'],\n",
       " ['Week', 'Weekday', 'Time'],\n",
       " ['Quarter', 'Day', 'Time', 'Weekday']]"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the feature containing all combinations i.e same as input feature list\n",
    "\n",
    "featureIndex = []\n",
    "for ix,i in enumerate(independentFeatures):\n",
    "    if(i in features):\n",
    "        featureIndex.append(ix)\n",
    "        \n",
    "def removeInputFeatures(independentFeatures):\n",
    "    for ix,i in enumerate(independentFeatures):\n",
    "        if(i in features):\n",
    "            independentFeatures.pop(ix)\n",
    "            \n",
    "for i in range(len(featureIndex)):\n",
    "    removeInputFeatures(independentFeatures)\n",
    "    \n",
    "print(\"After removing those features that are same as originalset of input features\")\n",
    "independentFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinations of input features that are independent of each other and dependent on target variable, which can be considered while modelling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['DateTime', 'Weekday', 'Time'],\n",
       " ['Junction', 'Day', 'Time', 'Weekday'],\n",
       " ['Month', 'Weekday', 'Time'],\n",
       " ['Day', 'Junction', 'Weekday', 'Quarter', 'Time'],\n",
       " ['Week', 'Weekday', 'Time'],\n",
       " ['Quarter', 'Day', 'Time', 'Weekday']]"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "featuresForModelling : These features can be considered while creating models.\n",
    "These are combinations of input features that are independent of each other and dependent on target variable\n",
    "\"\"\"\n",
    "featuresForModelling = independentFeatures\n",
    "print(\"Combinations of input features that are independent of each other and dependent on target variable, which can be considered while modelling\")\n",
    "featuresForModelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
